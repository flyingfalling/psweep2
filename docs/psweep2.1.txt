TODO:

## DO LATER 1) Force user to specify "data" files at beginning. They will be copied to workers. Do this later, just do everything sending now. Easier.

## DO LATER 2) Go through and swap data file names in the CMD first. Keep a marker noting which have swapped so we don't accidentally re-do same by accident.

3) Go through and swap out actual swaps

## 4) (In future -- figure out how to make canonical filename forms)

5) Make it so that I send files to the target worker along with "relocated" filenames. Only send those which are specified as "required" but not data files
(REV: fuck they might overlap)

## (REV ITS INPUT FILE )6) Where do I print out parampoint etc.? It's an automatic required guy. Meh it's sent with the varset anyway. Just print it out on other side?

7) OK, so I literally send the whole PARAMPOINT to the target. All REQUIRED files are also sent as it is expecting to receive them. It then prints
and renames them appropriately, one by one, remembering how it renamed them. This is the NEW/OLD list.
This list is then used to modify the PARAMPOINT locally on the client side.

How to deal with "success" files? They will be specified in MASTER space, need it in WORKER space. Make it so that they are always referenced in some
kind of reference to local TMP location? Assume they are ALWAYS produced in PITEM directory? And re-build the PARAMPOINT there I guess? OK. I may have
even passed it the names of the success files (ugh), i.e. told CMD where to write the success files... in which case...? That's shitty, I need to
rebuild the whole parampoint after all with the different guy. And the variables will be set properly with the new basis, but only if they refer to
SUCCESS or REQUIRED files? No, only if they build on the MYDIR etc.

Ugh, 2 types, 1 where I specify what the SUCCESS file is (i.e. tell user program where to print it), another where user program outputs to some FNAME,
e.g. I gave it dir, and I have to tell MY guy what that thing's name will be. Need to handle BOTH cases?! Ugly as shit. I should swap them both. I.e.
rebuild it with different basis, but only use um, new user base. Make sure (force?) all success files to be in PITEM dir? Input files may not be though.
Fine, I rebuild everything, and only regenerate SUCCESS file names. Then, I replace old ones with new ones in CMD (if they occur).
That seems only way to do it. Again, am I forcing user to "build" his SUCCESS filenames or something from the base dir of this PITEM thing?
But, some SUCCESS filenames would not have been regenerated. I can detect those, and appropriately replace those. Even better option  is to have
everything set as a "relative" filename to something, i.e. they are not strings, but strings inside something.

In which case adding a "success" file will specify INSIDE MYDIR FILE. OK. (usually)
Similarly, required files will do that too? Nah, but then what if it is in previous guy? Or in data guy? OK.

So, do success files. And then, I need to somehow copy it back appropriately...? I.e. to correct locations? Well I just literally copy them to the
corresponding point (filename) in that PITEM dir in MASTER. I.e. I just transfer between the locations in MASTER and SLAVE. Not full file shit.
For required though, I do copy and rename to solve problems with renaming/clashes.

In some
crazy case, it is just some arbitrary filename. Nah, don't allow that to happen? We just change base dir? Nah. Fuck it, we need to in some sense always
have the filename in terms of some BASE, i.e. never allow it to be a global thing. But user might want to do that? If he's a little cocksucker. OK.
So, force all SUCCESS files to be in terms of some BASE of this parampoint (or of this SWEEP). And always print out a correspondence file for hell of it.
Always interpret success files in that sense. I.e. build the checks at SUCCESS check time? No, they must also be built for possibly passing into CMD!!!
So, yes we must rebuild the whole thing... If the rebuild doesn't appropriately rebuild the SUCCESS files, we have a problem (?).



What was I doing? What do I need to do? At any rate, I need to figure out how to move SUCCESS and REQUIRED files. REQUIRED I have it figured out,
SUCCESS just assume it is always in the target.

I.e. I pass the PARAMPOINT. OK. I reconstruct the parampoint with __MY_DIR somehow reset?

Need to automatically set commands like __MY_DIR? Shit, each one needs to have it set along the way.

OK, If I reconstruct it we'll assume all sorts of shit? Haha Richard, just forget it. Assume user ALWAYS adds SUCCESS files inside __MY_DIR. So,
I can expect everything in there. We need to reconstruct everything. Furthermore, we can pass it through, and then reconstruct it, and swap out
the first part of the filename for the proper one in the SLAVE, and build the correspondences that way. That's the "easiest" way to do it.
But, what if it like, builds some sub-folders in there? Or some shit? Meh.





So MAIN IDEA (TODO)

1) transfer the PARAMPOINT. Re-generate the parameter point INSIDE the new location (i.e. just feed it the new base dir). This will construct the
   whole directory structure, which is actually unnecessary lol. So, don't do this actually...just make a new dir arbitrarily. But, if we don't
   reconstruct it we will have some problems (?). Just construct the last level, the last DIR? Note it is just this PITEM. Can I just construct a
   PITEM? Sure, but it requests the PARENT guy I think. Best to just modify the existing PARAMPOINT (PITEM) (specifically CMD and SUCCESS/REQUIRED list)
   and run it that way. Specifically we know the new guys, as we use same FILENAME, but change BASE FILE for SUCCESS. For REQUIRED list, we simply
   rename them. Then we go through and replace all instances of these files in CMD and SUCCESS/REQUIRED file array.

1) Transfer the whole parampoint struct. Set (temporarily?) the MYDIR of the desired PITEM to a temporarily created filename. I then execute (on
   this side) a "receive" for the desired files, which are pushed one by one? No, they are already remade on opposite (MASTER?) side? Waste...do it as
   slave. Send them and have slave name them as I go. I don't want to make a bunch of dires. Anyway, just get PITEM, and set its MYDIR to that? Need
   to keep hierarch varlist. PITEM will have the SUCCESS and REQUIRED files etc. I pretty much just need to rename those, no need to get into
   the VARLIST at all!!! *SO* much nicer to have some function to reliably convert everything from source MASTER to slave SLAVE.

   Specifically, for a given PITEM, if source has 3 things to change:
   //REQUIRED file list
   //SUCCESS file list
   //OUTPUT file list (or do it on other side)
   //INPUT file (name) just a single one.
   //MYDIR string (just a single one)
   //MYCMD, a vector of strings, will be concat with some "SEP" at the end.
   
2) For each file [Z] in REQUIRED, a) generate a new arbitrary filename __MY_DIR/required_files/fileZ, and set the correspondence OLDFNAME, NEWFNAME
3) For each file [Z] in SUCCESS, a) check if it is outside the legal __MY_DIR location (error if so), b) 
4) 





## 8) Final question is how to deal with namespace stuff, i.e. global variables? Whatever, just pass CMD to SYSTEM().

## 9) need to write to HDF5. At any way start it running using old script, while writing new guy, which will pass it as memory. Also write GPU stuff.


REV: Ideas for final update of PSWEEP2, which allows hybrid calls to SYSTEM() as well as calls to existing (C) functions provided by user program.


For example, user writes a program MYPROG.cpp, which includes the PSWEEP headers. PSWEEP simply provides an interface so that user can run his functions.
Unfortunately, this requires user to also help at CMD time, by parsing/passing PSWEEP specific variables/settings. Which messes some things up. Oh well.

At any rate, idea is that files can be loaded into variables as "binary" format, and saved as variables named by strings. So, we can specify to pass
these variables' content to the user specified functions.

The only difference now is that CMD may not be a CMD, but rather may specify a function name (string?) that the user has to register? Nah, that is
unrealistic. User function may be of arbitrary signature, but assume we force it to take void* or list of void* containing the input data? Pain
in the ass, but so much faster... OK. Solves some problems with GPU as well?

We still need to tell the "worker" what to call. I.e. we pass to it the command. And we need to also pass other important variables such as,
which GPU to use etc.? Distribute those first, i.e. they are set at initialization... Fine.

Still doesn't solve the problem of how to specify actual function to call via the text script that user passes.
Easiest way is to force user to make specifically named "structs" that contain the function calls. And add them to some "caller" struct. And
then the user will just say "call caller thing #2" or "call caller thing #1", and I know what to do with it then. I.e. since user is writing some
stuff in the C side, I let them do that. Fine. I.e. instead of setting CMD, I set nothing.

I can force other "input", other than input files, but require "input memory" too? I really don't want to go through NFS if possible...

OK, that works out. Man unfortunately matlab etc. will be PAINFUL. All files etc.? With new PASCAL GPU, we can do multi-GPU etc. easier?


Whatever, so I need a regime that lets me do this stuff easier. Need a way to read in arbitrary info from files to variables. I can do that. Then pass
those variables as memory. I like that idea? Heck, I can do that, but can't pass with SYSTEM(). Whatever.

Make sure I leave a way to do everything either way.

How to pass params? Well, it doesn't know anything about params. Just variables? OK. At any rate we need ways to set model parameters as well? For
different models. So we need ways to pass NAMED (i.e. global) variables conditional on being of correct model.


At any rate, right now we need to handle /TMP stuff properly. Best way is to go through CMD and replace. But, that is not a solution. We really need
to do it for everything? CMD or other guy e.g. if it is some other command for calling e.g. C++ program directly, but requires files (that is only
case where it would matter, if passing memory we would do it differently I assume).






    //I.e. ADD_REQ_DAT_FILE( fname ), i.e. it will find that filename in data folder in current guy. But that doesn't exist...

    //But that will not be re-run at CMD time on worker, i.e. without reconstructing it. That's a problem. So, I still need to trade out
    //all data file names with new corresponding location...pain in the butt. I.e. I don't want to copy those over they should already be there.
    //So I need to know that somehow? I.e. have a separate "required data files" and "required non-data files". Again, impossible/pain in the butt.
    //Furthermore, success checks need to all be changed.

    //Still I don't want to send the stuff across again if I don't have to (for DAT files)
    //In which case, I have to have a specification if which ones are DAT files? So for those one's I have a separate "required" list? but if it's a
    //separate required list, user needs to specify separately which are "data" etc. How about, add separate DATA files, make new guy, and make sure
    //those are not sent across the network. Just search for that filename in the DATA file of the new guy. I.e. actually modify the data name. Assume
    //he doesn't have weird shit like ../blah/../blah/.. etc. I.e. going back and forth in dirs. up/down in dirs

    //REV: whatever, just do it. Specifically:
    //at runtime, have a "dat" file list (of required dat files that I check anyway), those are specified at beginning. If user messes up and
    //sets dat file as required file, whatever, it will be copied anyway and wasteful.
    //Have one thing at beginning to "send" (broadcast) dat files to all guys simultaneously and save to specific folder. Great. Then user
    //specifies required dat files, but "correspondences" that I set are included in the ORIG and NEW (if it is in data folder (?), I set it to
    //worker's data folder.) I.e. I'm iterating through the command, first swapping out all "data" guys, then all "required" guys. Hopefully they're
    //not going back and forth or shit. I could make a correspondence, like an isomorphism of graph so that a/../a/../a is same as just ./a or something.
    //I.e. canonical form.

    //We need a global "data" file thing that I check. Note it is all copied at "beginning" somehow, i.e. user sets that up in some (other) script I
    //guess. Only purpose is to build the "DATA" file list.
    //Do something similar for the program code, i.e. compile it on each worker and check? For MPI that works, but we need to do it on each machine
    //at beginning. Hope it works. I.e. need some script...this is important if each one uses a different processor thing? But MPIC++ should do that
    //for me I guess. But for each WORKER program, it does need to be recompield because its not the MPI code that's the model. Fine.
    //set that stuff up?

    //So, actually how to impl?

    //Just set up DATA. And then we build the correspondence, we do a "send" of the non-data guys. ANd we find them all. We likewise search and
    //find all files in "DATA" there. Problem is how to send the "data" file (array) i.e. specification to target. Needs to be something shared
    //by all PARAMPOINTS, thus, it is like a "named" type guy. It could be stored in VARs but it could also not...? Need a simple function to "canonicalize"
    //all file names/file paths. This stuff is all for files. Doesn't matter if we're dealing with non-files.

    //What is everything sent as? What if there is "data" files that are e.g. in some format other than "variables". We don't want to convert to
    //string, just send as a bit chunk? Is that necessary? I.e. representation of a network in binary format (wow might be different between
    //machines though). Fine. So what TYPE to send it as? I don't want to convert it to a variable. Need some "struct" that I will always expect to
    //receive from the "communicating" MPI program thing. That I'm calling.

    //No idea what the actual format of that struct would be though? Contain arbitrary data that I have no idea of, that is communicated to it. That must
    //work as blobs, i.e. void*. Just bytes. And user program will cast it to their appropriate type I guess...ugh.


    //REV: OK DO IT. We have at beginning the DAT stuff, the INIT script for compilation, checking of MD5 etc.
    //And finally the method for switching between workers.

    //Furthermore, we must have a (global) DAT type thing which is always passed, that we use to search for DAT?
    //User must explicitly reference cetain files as DAT? Or just required files? I like the required files Idea. And when we do a "transform"
    //of the required files, some of them are DAT, so we check those first of course. Nono, we just transform DAT first.
    //There is also "source" files, which are transferred at the VERY beginning, and compiled etc. We need to keep track of stuff like
    //the MD5 of thesource so we know when we have changed? Better to do stuff like diffs so that we literally know the functional changes, i.e.
    //line numbers etc.?

    //I also want to have a way so that (eventually) I can specify filenames of "chunks" that can be passed just that way (as memory) to the target.
    //I.e. these "variables" are stored as literal void pointers of a certain length, and it is up to the child program to parse them. We can keep
    //MD5 sums etc. around to make sure they are sane. This way we only have to keep track of one? We could keep them "local" after broadcasting them
    //to each worker though (so that we don't have to do MPI distribute each time).

    //That's fine figure it out after. At any rate, give it access to files too.

    //Have STRING-STRING but also STRING-ARRAY of arbitrary CHAR, i.e. arbitrary data haha. Why not just stransfer a "filesystem in memory"
    //That's fine. Could do it with HDF5 struct to make it so that it doesn't matter what client CPU is?

    //What is the point of the scripts I write? It's to get correct file names/variables etc. for the client user program without having to pass it
    //directly to it. Is there any point of having the scripts if I literally am just running models?

    //For example if I want it to tell certain functions to run. Well I have to write C++ or whatever to use the program itself so at that point
    //I might as well just tell it. Easier to set some variables etc.? Man pain in the butt to specify which guys to use to load in to memory
    //though?

    //Remember we might want a hybrid approach...although forcing user to make his e.g. R programs run with my input might be an even "better" way
    //to speed stuff up. But it will require (dynamic?) linking at runtime I assume.

    //User might use any arbitrary language though...python, etc. So, let them do that, but best to not...

    //Also for stuff like, running on GPUs, it might be a pain in the ass. If we want to run multiple kernels in same GPU, we need to use the same
    //"rank" to spin them off. Pain in the ass haha.

    //At any rate
